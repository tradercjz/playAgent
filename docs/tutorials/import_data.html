<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh" lang="zh" data-whc_version="26.0">
    <head><link rel="shortcut icon" href="../favicon.ico"/><link rel="icon" href="../favicon.ico"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="description" content="DolphinDB提供了多种灵活的数据导入方法，来帮助用户方便的把海量数据从多个数据源导入。具体有如下4种途径： 通过文本文件导入 通过二进制文件导入 通过HDF5接口导入 通过ODBC接口导入 本章中多处使用到DolphinDB的数据库和表的概念，所以这里首先做一个介绍。 在DolphinDB里数据以结构化数据表的方式保存。数据表按存储介质可以分为： ..."/><meta name="DC.rights.owner" content="(C) 版权 2025"/><meta name="copyright" content="(C) 版权 2025"/><meta name="generator" content="DITA-OT"/><meta name="DC.type" content="topic"/><meta name="DC.format" content="HTML5"/><meta name="DC.identifier" content="数据导入概述"/><title>数据导入概述</title><!--  Generated with Oxygen version 26.0, build number 2024012323.  --><meta name="wh-path2root" content="../"/><meta name="wh-toc-id" content="&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;&lt;?workdir /tmp/temp20250305183303418/tutorials?&gt;&lt;?workdir-uri file:/tmp/temp20250305183303418/tutorials/?&gt;&lt;?path2project ../?&gt;&lt;?path2project-uri ../?&gt;&lt;?path2rootmap-uri ../?&gt;&lt;topic xmlns:dita-ot=&#34;http://dita-ot.sourceforge.net/ns/201007/dita-ot&#34; xmlns:ditaarch=&#34;http://dita.oasis-open.org/architecture/2005/&#34; class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;数据导入概述&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:1;1:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:1;1:1&#34;&gt;数据导入概述&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:1;1:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:1;3:1&#34;&gt;DolphinDB提供了多种灵活的数据导入方法，来帮助用户方便的把海量数据从多个数据源导入。具体有如下4种途径：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:1;5:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:1;5:1&#34;&gt;通过文本文件导入&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:2;6:1&#34;&gt;通过二进制文件导入&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:3;7:1&#34;&gt;通过HDF5接口导入&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:4;8:1&#34;&gt;通过ODBC接口导入&lt;/li&gt;&lt;/ul&gt;&lt;/body&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;1-dolphindb数据库基本概念和特点&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:2;10:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:2;10:1&#34;&gt;1. DolphinDB数据库基本概念和特点&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:2;10:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:2;12:1&#34;&gt;本章中多处使用到DolphinDB的数据库和表的概念，所以这里首先做一个介绍。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:3;14:1&#34;&gt;在DolphinDB里数据以结构化数据表的方式保存。数据表按存储介质可以分为：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:2;16:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:5;16:1&#34;&gt;内存表：数据保存在内存中，存取速度最快，但是若节点关闭就会丢失数据。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:6;17:1&#34;&gt;分布式表：数据分布在不同的节点的磁盘上，通过DolphinDB的分布式计算引擎，逻辑上仍然可以像本地表一样做统一查询。&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:4;19:1&#34;&gt;按是否分区可以分为：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:3;21:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:7;21:1&#34;&gt;普通表（未分区表）&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:8;22:1&#34;&gt;分区表&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:5;24:1&#34;&gt;在传统的数据库系统，分区是针对数据表定义的，就是同一个数据库里的每个数据表都可以有自己的分区定义；而DolphinDB的分区是针对数据库定义的，也就是说同一个数据库下的数据表只能使用同一种分区机制，这也意味着如果两张表要使用不同的分区机制，那么它们是不能放在一个数据库下的。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;2-通过文本文件导入&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:3;26:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:3;26:1&#34;&gt;2. 通过文本文件导入&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:3;26:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:6;28:1&#34;&gt;通过文件进行数据中转是比较通用化的一种数据迁移方式，方式简单易操作。DolphinDB提供了以下三个函数来载入文本文件：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:4;30:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:9;30:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:1;30:3&#34;&gt;loadText&lt;/codeph&gt;: 将文本文件以 DolphinDB 数据表的形式读取到内存中。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:10;31:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:2;31:3&#34;&gt;ploadText&lt;/codeph&gt;: 将数据文件作为分区表并行加载到内存中。与&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:3;31:36&#34;&gt;loadText&lt;/codeph&gt;函数相比，速度更快。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:11;32:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:4;32:3&#34;&gt;loadTextEx&lt;/codeph&gt;: 把数据文件转换为DolphinDB数据库中的分布式表，然后将表的元数据加载到内存中。&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:7;34:1&#34;&gt;下面通过将 &lt;xref class=&#34;- topic/xref &#34; href=&#34;data/candle_201801.csv&#34; format=&#34;csv&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:1;34:7&#34;&gt;&lt;?ditaot usertext?&gt;candle_201801.csv&lt;/xref&gt; 导入DolphinDB来演示&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:5;34:65&#34;&gt;loadText&lt;/codeph&gt;和&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:6;34:76&#34;&gt;loadTextEx&lt;/codeph&gt;的用法。&lt;/p&gt;&lt;/body&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;21-loadtext&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:4;36:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:4;36:1&#34;&gt;2.1. &lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:7;36:10&#34;&gt;loadText&lt;/codeph&gt;&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:4;36:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:8;38:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:8;38:1&#34;&gt;loadText&lt;/codeph&gt;函数有三个参数，第一个参数filename是文件名，第二个参数delimiter用于指定不同字段的分隔符，默认是&#34;,&#34;，第三个参数schema是用来指定导入后表的每个字段的数据类型，schema参数是一个数据表，格式示例如下：&lt;/p&gt;&lt;table class=&#34;- topic/table &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;table:1;40:1&#34;&gt;&lt;tgroup class=&#34;- topic/tgroup &#34; cols=&#34;2&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;tgroup:1;40:1&#34;&gt;&lt;colspec class=&#34;- topic/colspec &#34; colname=&#34;col1&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;colspec:1;40:1&#34; colnum=&#34;1&#34;/&gt;&lt;colspec class=&#34;- topic/colspec &#34; colname=&#34;col2&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;colspec:2;40:1&#34; colnum=&#34;2&#34;/&gt;&lt;thead class=&#34;- topic/thead &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;thead:1;40:1&#34;&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:1;40:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:1;40:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;1&#34;&gt;name&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:2;40:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;1&#34;&gt;type&lt;/entry&gt;&lt;/row&gt;&lt;/thead&gt;&lt;tbody class=&#34;- topic/tbody &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;tbody:1;42:1&#34;&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:2;42:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:3;42:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;2&#34;&gt;timestamp&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:4;42:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;2&#34;&gt;SECOND&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:3;43:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:5;43:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;3&#34;&gt;ID&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:6;43:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;3&#34;&gt;INT&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:4;44:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:7;44:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;4&#34;&gt;qty&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:8;44:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;4&#34;&gt;INT&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:5;45:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:9;45:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;5&#34;&gt;price&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:10;45:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;5&#34;&gt;DOUBLE&lt;/entry&gt;&lt;/row&gt;&lt;/tbody&gt;&lt;/tgroup&gt;&lt;/table&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:9;47:1&#34;&gt;首先导入数据：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:1;49:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.csv&#34; tmpTB = loadText(dataFilePath);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:10;54:1&#34;&gt;DolphinDB在导入数据的同时，随机提取一部分的行以确定各列数据类型，所以对大多数文本文件无须手动指定各列的数据类型，非常方便。但有时系统自动识别的数据类型并不符合预期或需求，比如导入数据的volume列被识别为INT类型, 而需要的volume类型是LONG类型，这时就需要使用一个数据类型表作为schema参数。例如可使用如下脚本构建数据类型表：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:2;56:1&#34;&gt;nameCol = `symbol`exchange`cycle`tradingDay`date`time`open`high`low`close`volume`turnover`unixTime typeCol = `SYMBOL`SYMBOL`INT`DATE`DATE`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE`LONG schemaTb = table(nameCol as name,typeCol as type);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:11;62:1&#34;&gt;当表字段非常多的时候，写这样一个脚本费时费力，为了简化操作，DolphinDB提供了&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:9;62:43&#34;&gt;extractTextSchema&lt;/codeph&gt; 函数，可从文本文件中提取表的结构生成数据类型表。只需修改少数指定字段的数据类型，就可得到理想的数据类型表。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:12;64:1&#34;&gt;整合上述方法，可使用如下脚本以导入数据：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:3;66:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.csv&#34; schemaTb=extractTextSchema(dataFilePath) update schemaTb set type=`LONG where name=`volume tt=loadText(dataFilePath,,schemaTb);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:13;73:1&#34;&gt;如果数据文件中包含时间、日期的数据，满足分隔符要求的这部分数据（日期数据分隔符包含&#34;-&#34;、&#34;/&#34;和&#34;.&#34;，时间数据分隔符为&#34;:&#34;）会解析为相应的类型。例如：&#34;10:56:16&#34;被解析为SECOND，&#34;2023-11-08&#34;被解析为DATE类型。对于不包含分隔符的数据，形如&#34;yyMMdd&#34;的数据同时满足0&amp;lt;=yy&amp;lt;=99，0&amp;lt;=MM&amp;lt;=12，1&amp;lt;=dd&amp;lt;=31，会被优先解析成DATE；形如&#34;yyyyMMdd&#34;的数据同时满足1900&amp;lt;=yyyy&amp;lt;=2100，0&amp;lt;=MM&amp;lt;=12，1&amp;lt;=dd&amp;lt;=31会被优先解析成DATE。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:14;75:1&#34;&gt;以数据文件 test_time.csv 为例，date1-date8 不同格式的数据均被解析为 DATE 类型，second被解析为 SECOND 类型。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:4;77:1&#34;&gt;dataFilePath = &#34;/home/data/test_time.csv&#34; schemaTable = extractTextSchema(dataFilePath)&lt;/codeblock&gt;&lt;table class=&#34;- topic/table &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;table:2;82:1&#34;&gt;&lt;tgroup class=&#34;- topic/tgroup &#34; cols=&#34;2&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;tgroup:2;82:1&#34;&gt;&lt;colspec class=&#34;- topic/colspec &#34; colname=&#34;col1&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;colspec:3;82:1&#34; colnum=&#34;1&#34;/&gt;&lt;colspec class=&#34;- topic/colspec &#34; colname=&#34;col2&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;colspec:4;82:1&#34; colnum=&#34;2&#34;/&gt;&lt;thead class=&#34;- topic/thead &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;thead:2;82:1&#34;&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:6;82:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:11;82:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;1&#34;&gt;name&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:12;82:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;1&#34;&gt;type&lt;/entry&gt;&lt;/row&gt;&lt;/thead&gt;&lt;tbody class=&#34;- topic/tbody &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;tbody:2;84:1&#34;&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:7;84:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:13;84:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;2&#34;&gt;date1&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:14;84:11&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;2&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:8;85:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:15;85:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;3&#34;&gt;date2&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:16;85:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;3&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:9;86:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:17;86:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;4&#34;&gt;date3&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:18;86:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;4&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:10;87:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:19;87:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;5&#34;&gt;date4&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:20;87:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;5&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:11;88:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:21;88:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;6&#34;&gt;date5&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:22;88:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;6&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:12;89:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:23;89:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;7&#34;&gt;date6&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:24;89:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;7&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:13;90:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:25;90:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;8&#34;&gt;date7&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:26;90:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;8&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:14;91:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:27;91:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;9&#34;&gt;date8&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:28;91:14&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;9&#34;&gt;DATE&lt;/entry&gt;&lt;/row&gt;&lt;row class=&#34;- topic/row &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;row:15;92:1&#34;&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:29;92:1&#34; colname=&#34;col1&#34; dita-ot:x=&#34;1&#34; dita-ot:y=&#34;10&#34;&gt;second&lt;/entry&gt;&lt;entry class=&#34;- topic/entry &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;entry:30;92:11&#34; colname=&#34;col2&#34; dita-ot:x=&#34;2&#34; dita-ot:y=&#34;10&#34;&gt;SECOND&lt;/entry&gt;&lt;/row&gt;&lt;/tbody&gt;&lt;/tgroup&gt;&lt;/table&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:15;94:1&#34;&gt;为确保数据导入准确，需要需要在 format 列中指定数据文件中日期或时间的格式&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:5;96:1&#34;&gt;dataFilePath = &#34;/home/data/test_time.csv&#34; schemaTable = extractTextSchema(dataFilePath) formatColumn = [&#34;yyyy.MM.dd&#34;,&#34;yyyy/MM/dd&#34;,&#34;yyyy-MM-dd&#34;,&#34;yyyyMMdd&#34;,&#34;yy.MM.dd&#34;,&#34;yy/MM/dd&#34;,&#34;yy-MM-dd&#34;,&#34;yyMMdd&#34;,&#34;HH:mm:ss&#34;] schemaTable[`format] = formatColumn t = loadText(dataFilePath,',',schemaTable) ### 2.2. `ploadText` `ploadText`函数的特点可以快速载入大文件（至少16MB）。它在设计中充分利用了多核CPU来并行载入文件，并行程度取决于服务器本身CPU核数量和节点的workerNum配置。 首先通过脚本生成一个4G左右的CSV文件： ```txt filePath = &#34;/home/data/testFile.csv&#34; appendRows = 100000000 dateRange = 2010.01.01..2018.12.30 ints = rand(100, appendRows) symbols = take(string('A'..'Z'), appendRows) dates = take(dateRange, appendRows) floats = rand(float(100), appendRows) times = 00:00:00.000 + rand(60 * 60 * 24 * 1000, appendRows) t = table(ints as int, symbols as symbol, dates as date, floats as float, times as time) t.saveText(filePath)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:16;121:1&#34;&gt;分别通过&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:10;121:5&#34;&gt;loadText&lt;/codeph&gt;和&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:11;121:16&#34;&gt;ploadText&lt;/codeph&gt;来载入文件。本例所用节点是4核8超线程的CPU。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:6;123:1&#34;&gt;timer loadText(filePath); Time elapsed: 39728.393 ms timer ploadText(filePath); Time elapsed: 10685.838 ms&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:17;131:1&#34;&gt;结果显示在此配置下，&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:12;131:11&#34;&gt;ploadText&lt;/codeph&gt;的性能是&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:13;131:26&#34;&gt;loadText&lt;/codeph&gt;的4倍左右。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;23-loadtextex&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:5;133:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:5;133:1&#34;&gt;2.3. &lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:14;133:10&#34;&gt;loadTextEx&lt;/codeph&gt;&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:5;133:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:18;135:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:15;135:1&#34;&gt;loadText&lt;/codeph&gt;函数总是把所有数据导入内存。当数据文件体积非常庞大时，服务器的内存很容易成为制约因素。DolphinDB提供的&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:16;135:66&#34;&gt;loadTextEx&lt;/codeph&gt;函数可以较好的解决这个问题。它将一个大的文本文件分割成很多个小块，逐步加载到分布式数据表中。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:19;137:1&#34;&gt;首先创建分布式数据库：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:7;139:1&#34;&gt;db=database(&#34;dfs://dataImportCSVDB&#34;,VALUE,2018.01.01..2018.01.31) &lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:20;143:1&#34;&gt;然后将文本文件导入数据库中&#34;cycle&#34;表：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:8;145:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.csv&#34; loadTextEx(db, &#34;cycle&#34;, &#34;tradingDay&#34;, dataFilePath)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:21;150:1&#34;&gt;当需要使用数据时，通过&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:17;150:12&#34;&gt;loadTable&lt;/codeph&gt;函数将分区元数据先载入内存。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:9;152:1&#34;&gt;tb = database(&#34;dfs://dataImportCSVDB&#34;).loadTable(&#34;cycle&#34;)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:22;156:1&#34;&gt;在实际执行查询的时候，会按需加载所需数据到内存。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;3-通过二进制文件导入&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:6;158:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:6;158:1&#34;&gt;3. 通过二进制文件导入&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:6;158:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:23;160:1&#34;&gt;对于二进制格式的文件，DolphinDB提供了2个函数用于导入：&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:18;160:33&#34;&gt;readRecord!&lt;/codeph&gt;函数和&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:19;160:49&#34;&gt;loadRecord&lt;/codeph&gt;函数。二者的区别是，前者不支持导入字符串类型的数据，后者支持。下面通过2个例子分别介绍这两个函数的用法。&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:5;162:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:12;162:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:20;162:3&#34;&gt;readRecord!&lt;/codeph&gt;函数&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:24;164:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:21;164:1&#34;&gt;readRecord!&lt;/codeph&gt;函数能够导入不含有字符串类型字段的二进制文件，下面介绍如何使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:22;164:45&#34;&gt;readRecord!&lt;/codeph&gt;函数导入一个二进制文件：&lt;xref class=&#34;- topic/xref &#34; href=&#34;data/binSample.bin&#34; format=&#34;bin&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:2;164:70&#34;&gt;&lt;?ditaot usertext?&gt;binSample.bin&lt;/xref&gt;。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:25;166:1&#34;&gt;首先，创建一个内存表tb，用于存放导入的数据，需要为每一列指定字段名称和数据类型。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:10;168:1&#34;&gt;tb=table(1000:0, `id`date`time`last`volume`value`ask1`ask_size1`bid1`bid_size1, [INT,INT,INT,FLOAT,INT,FLOAT,FLOAT,INT,FLOAT,INT])&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:26;172:1&#34;&gt;调用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:23;172:3&#34;&gt;file&lt;/codeph&gt;函数打开文件，并通过&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:24;172:19&#34;&gt;readRecord!&lt;/codeph&gt;函数导入二进制文件，数据会被加载到tb表中。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:11;174:1&#34;&gt;dataFilePath=&#34;/home/data/binSample.bin&#34; f=file(dataFilePath) f.readRecord!(tb);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:27;180:1&#34;&gt;查看tb表的数据，数据已经正确导入：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:12;182:1&#34;&gt;select top 5 * from tb; id date time last volume value ask1 ask_size1 bid1 bid_size1 -- -------- -------- ---- ------ ----- ----- --------- ----- --------- 1 20190902 91804000 0 0 0 11.45 200 11.45 200 2 20190902 92007000 0 0 0 11.45 200 11.45 200 3 20190902 92046000 0 0 0 11.45 1200 11.45 1200 4 20190902 92346000 0 0 0 11.45 1200 11.45 1200 5 20190902 92349000 0 0 0 11.45 5100 11.45 5100&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:28;194:1&#34;&gt;date列和time列的数据为INT类型。可以使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:25;194:26&#34;&gt;temporalParse&lt;/codeph&gt;函数进行日期和时间类型数据的格式转换，再使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:26;194:63&#34;&gt;replaceColumn!&lt;/codeph&gt;函数替换表中原有的列。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:13;196:1&#34;&gt;tb.replaceColumn!(`date, tb.date.string().temporalParse(&#34;yyyyMMdd&#34;)) tb.replaceColumn!(`time, tb.time.format(&#34;000000000&#34;).temporalParse(&#34;HHmmssSSS&#34;)) select top 5 * from tb; id date time last volume value ask1 ask_size1 bid1 bid_size1 -- ---------- ------------ ---- ------ ----- ----- --------- ----- --------- 1 2019.09.02 09:18:04.000 0 0 0 11.45 200 11.45 200 2 2019.09.02 09:20:07.000 0 0 0 11.45 200 11.45 200 3 2019.09.02 09:20:46.000 0 0 0 11.45 1200 11.45 1200 4 2019.09.02 09:23:46.000 0 0 0 11.45 1200 11.45 1200 5 2019.09.02 09:23:49.000 0 0 0 11.45 5100 11.45 5100&lt;/codeblock&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:6;210:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:13;210:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:27;210:3&#34;&gt;loadRecord&lt;/codeph&gt;函数&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:29;212:1&#34;&gt;&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:28;212:1&#34;&gt;loadRecord&lt;/codeph&gt;函数能够处理字符串类型的数据（包括STRING和SYMBOL类型），但是要求字符串在磁盘上的长度必须固定。如果字符串的长度小于固定值，则用ASCII值0填充，加载的时候会把末尾0去掉。下面介绍使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:29;212:111&#34;&gt;loadRecord&lt;/codeph&gt;函数导入一个带有字符串类型字段的二进制文件：&lt;xref class=&#34;- topic/xref &#34; href=&#34;data/binStringSample.bin&#34; format=&#34;bin&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:3;212:145&#34;&gt;&lt;?ditaot usertext?&gt;binStringSample.bin&lt;/xref&gt;。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:30;214:1&#34;&gt;首先，指定要导入文件的表结构，包括字段名称和数据类型。与&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:30;214:29&#34;&gt;readRecord!&lt;/codeph&gt;函数不同的是，&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:31;214:49&#34;&gt;loadRecord&lt;/codeph&gt;函数是通过一个元组来指定schema，而不是直接定义一个内存表。关于表结构的指定，有以下3点要求：&lt;/p&gt;&lt;ol class=&#34;- topic/ol &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ol:1;216:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:14;216:1&#34;&gt;对于表中的每个字段，都需要以tuple的形式指定字段名称和相应的数据类型。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:15;217:1&#34;&gt;若类型是字符串，还需指定磁盘上的字符串长度（包括结尾的0）。例如：（&#34;name&#34;,SYMBOL,24）。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:16;218:1&#34;&gt;将所有tuple按照字段顺序组成元组，作为表结构。&lt;/li&gt;&lt;/ol&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:31;220:1&#34;&gt;针对本例中的数据文件指定表结构，具体如下所示。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:14;222:1&#34;&gt;schema = [(&#34;code&#34;, SYMBOL, 32),(&#34;date&#34;, INT),(&#34;time&#34;, INT),(&#34;last&#34;, FLOAT),(&#34;volume&#34;, INT),(&#34;value&#34;, FLOAT),(&#34;ask1&#34;, FLOAT),(&#34;ask2&#34;, FLOAT),(&#34;ask3&#34;, FLOAT),(&#34;ask4&#34;, FLOAT),(&#34;ask5&#34;, FLOAT),(&#34;ask6&#34;, FLOAT),(&#34;ask7&#34;, FLOAT),(&#34;ask8&#34;, FLOAT),(&#34;ask9&#34;, FLOAT),(&#34;ask10&#34;, FLOAT),(&#34;ask_size1&#34;, INT),(&#34;ask_size2&#34;, INT),(&#34;ask_size3&#34;, INT),(&#34;ask_size4&#34;, INT),(&#34;ask_size5&#34;, INT),(&#34;ask_size6&#34;, INT),(&#34;ask_size7&#34;, INT),(&#34;ask_size8&#34;, INT),(&#34;ask_size9&#34;, INT),(&#34;ask_size10&#34;, INT),(&#34;bid1&#34;, FLOAT),(&#34;bid2&#34;, FLOAT),(&#34;bid3&#34;, FLOAT),(&#34;bid4&#34;, FLOAT),(&#34;bid5&#34;, FLOAT),(&#34;bid6&#34;, FLOAT),(&#34;bid7&#34;, FLOAT),(&#34;bid8&#34;, FLOAT),(&#34;bid9&#34;, FLOAT),(&#34;bid10&#34;, FLOAT),(&#34;bid_size1&#34;, INT),(&#34;bid_size2&#34;, INT),(&#34;bid_size3&#34;, INT),(&#34;bid_size4&#34;, INT),(&#34;bid_size5&#34;, INT),(&#34;bid_size6&#34;, INT),(&#34;bid_size7&#34;, INT),(&#34;bid_size8&#34;, INT),(&#34;bid_size9&#34;, INT),(&#34;bid_size10&#34;, INT)]&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:32;226:1&#34;&gt;使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:32;226:3&#34;&gt;loadRecord&lt;/codeph&gt;函数导入二进制文件，由于表的列数较多，通过select语句选出几列有代表性的数据进行后面的介绍。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:15;228:1&#34;&gt;dataFilePath=&#34;/home/data/binStringSample.bin&#34; tmp=loadRecord(dataFilePath, schema) tb=select code,date,time,last,volume,value,ask1,ask_size1,bid1,bid_size1 from tmp;&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:33;234:1&#34;&gt;查看表内数据的前5行：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:16;236:1&#34;&gt;select top 5 * from tb; code date time last volume value ask1 ask_size1 bid1 bid_size1 --------- -------- -------- ---- ------ ----- ----- --------- ----- --------- 601177.SH 20190902 91804000 0 0 0 11.45 200 11.45 200 601177.SH 20190902 92007000 0 0 0 11.45 200 11.45 200 601177.SH 20190902 92046000 0 0 0 11.45 1200 11.45 1200 601177.SH 20190902 92346000 0 0 0 11.45 1200 11.45 1200 601177.SH 20190902 92349000 0 0 0 11.45 5100 11.45 5100&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:34;248:1&#34;&gt;处理日期和时间列的数据：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:17;250:1&#34;&gt;tb.replaceColumn!(`date, tb.date.string().temporalParse(&#34;yyyyMMdd&#34;)) tb.replaceColumn!(`time, tb.time.format(&#34;000000000&#34;).temporalParse(&#34;HHmmssSSS&#34;)) select top 5 * from tb; code date time last volume value ask1 ask_size1 bid1 bid_size1 --------- ---------- ------------ ---- ------ ----- ----- --------- ----- --------- 601177.SH 2019.09.02 09:18:04.000 0 0 0 11.45 200 11.45 200 601177.SH 2019.09.02 09:20:07.000 0 0 0 11.45 200 11.45 200 601177.SH 2019.09.02 09:20:46.000 0 0 0 11.45 1200 11.45 1200 601177.SH 2019.09.02 09:23:46.000 0 0 0 11.45 1200 11.45 1200 601177.SH 2019.09.02 09:23:49.000 0 0 0 11.45 5100 11.45 5100&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:35;264:1&#34;&gt;除了&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:33;264:3&#34;&gt;readRecord!&lt;/codeph&gt;和&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:34;264:17&#34;&gt;loadRecord&lt;/codeph&gt;函数之外，DolphinDB还提供了一些与二进制文件的处理相关的函数，例如&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:35;264:66&#34;&gt;writeRecord&lt;/codeph&gt;函数，用于将DolphinDB对象保存为二进制文件。具体请参考用户手册。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;4-通过hdf5接口导入&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:7;266:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:7;266:1&#34;&gt;4. 通过HDF5接口导入&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:7;266:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:36;268:1&#34;&gt;HDF5是一种高效的二进制数据文件格式，在数据分析领域广泛使用。DolphinDB支持导入HDF5格式数据文件。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:37;270:1&#34;&gt;DolphinDB通过HDF5插件来访问HDF5文件，插件提供了以下方法：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:7;272:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:17;272:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:38;272:3&#34;&gt;hdf5::ls - 列出h5文件中所有 Group 和 Dataset 对象&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:18;274:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:39;274:3&#34;&gt;hdf5::lsTable - 列出h5文件中所有 Dataset 对象&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:19;276:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:40;276:3&#34;&gt;hdf5::hdf5DS - 返回h5文件中 Dataset 的元数据&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:20;278:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:41;278:3&#34;&gt;hdf5::loadHDF5 - 将h5文件导入内存表&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:21;280:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:42;280:3&#34;&gt;hdf5::loadHDF5Ex - 将h5文件导入分区表&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:22;282:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:43;282:3&#34;&gt;hdf5::extractHDF5Schema - 从h5文件中提取表结构&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:44;284:1&#34;&gt;DolphinDB 1.00.0版本之后，安装目录/server/plugins/hdf5已经包含HDF5插件，使用以下脚本加载插件：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:18;286:1&#34;&gt;loadPlugin(&#34;plugins/hdf5/PluginHdf5.txt&#34;)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:45;290:1&#34;&gt;若用户使用的是老版本，默认不包含此插件，可先从&lt;xref class=&#34;- topic/xref &#34; href=&#34;../plugins/hdf5/hdf5.md&#34; dita-ot:orig-format=&#34;md&#34; format=&#34;dita&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:4;290:24&#34; type=&#34;topic&#34;&gt;&lt;?ditaot usertext?&gt;HDF5插件&lt;/xref&gt;对应版本分支bin目录下载，再将插件部署到节点的plugins目录下。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:46;292:1&#34;&gt;调用插件方法时需要在方法前面提供namespace，比如调用loadHDF5可以使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:36;292:43&#34;&gt;hdf5::loadHDF5&lt;/codeph&gt;。另一种写法是：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:19;294:1&#34;&gt;use hdf5 loadHDF5(filePath,tableName)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:47;299:1&#34;&gt;HDF5文件的导入与CSV文件类似。例如，若要导入包含一个Dataset candle_201801的文件candle_201801.h5，可使用以下脚本，其中datasetName可通过ls或lsTable获得：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:20;301:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.h5&#34; datasetName = &#34;candle_201801&#34; tmpTB = hdf5::loadHDF5(dataFilePath,datasetName)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:48;307:1&#34;&gt;如果需要指定数据类型导入可以使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:37;307:17&#34;&gt;hdf5::extractHDF5Schema&lt;/codeph&gt;，脚本如下：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:21;309:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.h5&#34; datasetName = &#34;candle_201801&#34; schema=hdf5::extractHDF5Schema(dataFilePath,datasetName) update schema set type=`LONG where name=`volume tt=hdf5::loadHDF5(dataFilePath,datasetName,schema)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:49;317:1&#34;&gt;如果HDF5文件超过服务器内存，可以使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:38;317:21&#34;&gt;hdf5::loadHDF5Ex&lt;/codeph&gt;载入数据。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:50;319:1&#34;&gt;首先创建用于保存数据的分布式表：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:22;321:1&#34;&gt;dataFilePath = &#34;/home/data/candle_201801.h5&#34; datasetName = &#34;candle_201801&#34; dfsPath = &#34;dfs://dataImportHDF5DB&#34; db=database(dfsPath,VALUE,2018.01.01..2018.01.31) &lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:51;328:1&#34;&gt;然后导入HDF5文件：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:23;330:1&#34;&gt;hdf5::loadHDF5Ex(db, &#34;cycle&#34;, &#34;tradingDay&#34;, dataFilePath,datasetName)&lt;/codeblock&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;5-通过odbc接口导入&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:8;334:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:8;334:1&#34;&gt;5. 通过ODBC接口导入&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:8;334:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:52;336:1&#34;&gt;DolphinDB支持ODBC接口连接第三方数据库，从其中直接将数据表读取成DolphinDB的内存数据表。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:53;338:1&#34;&gt;DolphinDB官方提供ODBC插件用于连接第三方数据源，使用该插件可以方便的从ODBC支持的数据库迁移数据至DolphinDB中。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:54;340:1&#34;&gt;ODBC插件提供了以下四个方法用于操作第三方数据源数据：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:8;342:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:23;342:1&#34;&gt;odbc::connect - 开启连接&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:24;343:1&#34;&gt;odbc::close - 关闭连接&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:25;344:1&#34;&gt;odbc::query - 根据给定的SQL语句查询数据并将结果返回到DolphinDB的内存表&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:26;345:1&#34;&gt;odbc::execute - 在第三方数据库内执行给定的SQL语句，不返回结果。&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:27;346:1&#34;&gt;odbc::append - 把DolphinDB中表的数据写入第三方数据库的表中。&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:55;348:1&#34;&gt;在使用ODBC插件之前，需要安装ODBC驱动程序，请参考&lt;xref class=&#34;- topic/xref &#34; href=&#34;../plugins/odbc/odbc.md&#34; dita-ot:orig-format=&#34;md&#34; format=&#34;dita&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:5;348:29&#34; type=&#34;topic&#34;&gt;&lt;?ditaot usertext?&gt;ODBC插件使用教程&lt;/xref&gt;。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:56;350:1&#34;&gt;下面的例子使用ODBC插件连接以下SQL Server：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:9;352:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:28;352:1&#34;&gt;server：172.18.0.15&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:29;353:1&#34;&gt;默认端口：1433&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:30;354:1&#34;&gt;连接用户名：sa&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:31;355:1&#34;&gt;密码：123456&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:32;356:1&#34;&gt;数据库名称： SZ_TAQ&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:57;358:1&#34;&gt;第一步，下载插件解压并拷贝 plugins\odbc 目录下所有文件到DolphinDB server的 plugins/odbc 目录下（有些版本的DolphinDB安装目录/server/plugins/odbc已经包含ODBC插件，可略过此步），通过下面的脚本完成插件初始化：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:24;360:1&#34;&gt;loadPlugin(&#34;plugins/odbc/odbc.cfg&#34;) conn=odbc::connect(&#34;Driver=ODBC Driver 17 for SQL Server;Server=172.18.0.15;Database=SZ_TAQ;Uid=sa;Pwd=123456;&#34;)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:58;365:1&#34;&gt;第二步，创建分布式数据库。使用SQL Server中的数据表结构作为DolphinDB数据表的模板。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:25;367:1&#34;&gt;tb = odbc::query(conn,&#34;select top 1 * from candle_201801&#34;) db=database(&#34;dfs://dataImportODBC&#34;,VALUE,2018.01.01..2018.01.31) db.createPartitionedTable(tb, &#34;cycle&#34;, &#34;tradingDay&#34;)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:59;373:1&#34;&gt;第三步，从SQL Server中导入数据并保存为DolphinDB分区表：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:26;375:1&#34;&gt;tb = database(&#34;dfs://dataImportODBC&#34;).loadTable(&#34;cycle&#34;) data = odbc::query(conn,&#34;select * from candle_201801&#34;) tb.append!(data);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:60;381:1&#34;&gt;通过ODBC导入数据方便快捷。通过DolphinDB的定时作业机制，它还可以作为时序数据定时同步的数据通道。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;6-导入数据实例&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:9;383:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:9;383:1&#34;&gt;6. 导入数据实例&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:9;383:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:61;385:1&#34;&gt;下面以股票市场日K线图数据文件导入作为示例。每个股票数据存为一个CSV文件，共约100G，时间范围为2008年-2017年，按年度分目录保存。2008年度路径示例如下：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:27;387:1&#34;&gt;2008 ---- 000001.csv ---- 000002.csv ---- 000003.csv ---- 000004.csv ---- ...&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:62;396:1&#34;&gt;每个文件的结构都是一致的，如图所示：&lt;/p&gt;&lt;image class=&#34;- topic/image &#34; href=&#34;images/csvfile.PNG&#34; placement=&#34;break&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;image:1;398:1&#34; dita-ot:image-width=&#34;853&#34; dita-ot:image-height=&#34;93&#34; dita-ot:horizontal-dpi=&#34;96&#34; dita-ot:vertical-dpi=&#34;96&#34;&gt;&lt;alt class=&#34;- topic/alt &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;alt:1;398:1&#34;&gt;csvfile&lt;/alt&gt;&lt;/image&gt;&lt;/body&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;61-分区规划&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:10;400:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:10;400:1&#34;&gt;6.1. 分区规划&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:10;400:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:63;402:1&#34;&gt;要导入数据之前，首先要做好数据的分区规划，即确定分区字段以及分区粒度。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:64;404:1&#34;&gt;确定分区字段要考虑日常的查询语句执行频率。以where, group by或context by中常用字段作为分区字段，可以极大的提升数据检索和分析的效率。使用股票数据的查询经常与交易日期和股票代码有关，所以我们建议采用 tradingDay和symbol这两列进行组合(COMPO)分区。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:65;407:1&#34;&gt;分区大小应尽量均匀，同时分区粒度不宜过大或过小。我们建议一个分区未压缩前的原始数据大小控制在100M~1G之间。有关为何分区大小应均匀，以及分区最佳粒度的考虑因素，请参考&lt;xref class=&#34;- topic/xref &#34; href=&#34;database.md&#34; dita-ot:orig-format=&#34;md&#34; format=&#34;dita&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:6;407:86&#34; type=&#34;topic&#34;&gt;&lt;?ditaot usertext?&gt;DolphinDB分区数据库教程&lt;/xref&gt;第四节。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:66;409:1&#34;&gt;综合考虑，我们可以在复合(COMPO)分区中，根据交易日期进行范围分区（每年一个范围），并按照股票代码进行范围分区（共100个代码范围），共产生 10 * 100 = 1000 个分区，最终每个分区的大小约100M左右。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:67;411:1&#34;&gt;首先创建交易日期的分区向量。若要为后续进入的数据预先制作分区，可把时间范围设置为2008-2030年。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:28;413:1&#34;&gt;yearRange = date(2008.01M + 12*0..22);&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:68;417:1&#34;&gt;通过以下脚本得到symbol字段的分区向量。由于每只股票的数据量一致，我们遍历所有的年度目录，整理出股票代码清单，并通过&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:39;417:61&#34;&gt;cutPoint&lt;/codeph&gt;函数分成100个股票代码区间。考虑到未来新增的股票代码可能会大于现有最大股票代码，我们增加了一个虚拟的代码999999，作为股票代码的上限值。&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:29;419:1&#34;&gt;symbols = array(SYMBOL, 0, 100) yearDirs = files(rootDir)[`filename] for(yearDir in yearDirs){ path = rootDir + &#34;/&#34; + yearDir symbols.append!(files(path)[`filename].upper().strReplace(&#34;.CSV&#34;,&#34;&#34;)) } symbols = symbols.distinct().sort!().append!(&#34;999999&#34;); symRanges = symbols.cutPoints(100)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:69;430:1&#34;&gt;通过以下脚本创建复合(COMPO)分区数据库，以及数据库内的分区表&#34;stockData&#34;：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:30;432:1&#34;&gt;columns=`symbol`exchange`cycle`tradingDay`date`time`open`high`low`close`volume`turnover`unixTime types = [SYMBOL,SYMBOL,INT,DATE,DATE,TIME,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG,DOUBLE,LONG] dbDate=database(&#34;&#34;, RANGE, yearRange) dbID=database(&#34;&#34;, RANGE, symRanges) db = database(dbPath, COMPO, [dbDate, dbID]) pt=db.createPartitionedTable(table(1000000:0,columns,types), `stockData, `tradingDay`symbol);&lt;/codeblock&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;62-导入数据&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:11;443:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:11;443:1&#34;&gt;6.2. 导入数据&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:11;443:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:70;445:1&#34;&gt;数据导入的具体过程是通过目录树，将所有的CSV文件读取并写入到分布式数据库表dfs://SAMPLE_TRDDB 中。这其中会有一些细节问题。例如，CSV文件中保存的数据格式与DolphinDB内部的数据格式存在差异，比如time字段，原始数据文件里是以整数例如“9390100000”表示精确到毫秒的时间， 如果直接读入会被识别成整数类型，而不是时间类型，所以这里需要用到数据转换函数&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:40;445:194&#34;&gt;datetimeParse&lt;/codeph&gt;结合格式化函数&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:41;445:216&#34;&gt;format&lt;/codeph&gt;在数据导入时进行转换。可采用以下脚本：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:31;447:1&#34;&gt;datetimeParse(format(time,&#34;000000000&#34;),&#34;HHmmssSSS&#34;)&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:71;451:1&#34;&gt;如果单线程导入100GB的数据会耗时很久。为了充分利用集群的资源，我们可以按照年度把数据导入拆分成多个子任务，发送到各节点的任务队列并行执行，提高导入的效率。这个过程可分为以下两步实现。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:72;453:1&#34;&gt;首先定义一个函数以导入指定年度目录下的所有文件：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:32;455:1&#34;&gt;def loadCsvFromYearPath(path, dbPath, tableName){ symbols = files(path)[`filename] for(sym in symbols){ filePath = path + &#34;/&#34; + sym t=loadText(filePath) database(dbPath).loadTable(tableName).append!(select symbol, exchange,cycle, tradingDay,date,datetimeParse(format(time,&#34;000000000&#34;),&#34;HHmmssSSS&#34;),open,high,low,close,volume,turnover,unixTime from t ) } }&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:73;466:1&#34;&gt;然后通过&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:42;466:5&#34;&gt;rpc&lt;/codeph&gt;函数结合&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:43;466:14&#34;&gt;submitJob&lt;/codeph&gt;函数把该函数提交到各节点去执行：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; outputclass=&#34;txt&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:33;468:1&#34;&gt;nodesAlias=&#34;NODE&#34; + string(1..4) years= files(rootDir)[`filename] index = 0; for(year in years){ yearPath = rootDir + &#34;/&#34; + year des = &#34;loadCsv_&#34; + year rpc(nodesAlias[index%nodesAlias.size()],submitJob,des,des,loadCsvFromYearPath,yearPath,dbPath,`stockData) index=index+1 }&lt;/codeblock&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:74;481:1&#34;&gt;数据导入过程中，可以使用&lt;codeph class=&#34;+ topic/ph pr-d/codeph &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeph:44;481:13&#34;&gt;pnodeRun(getRecentJobs)&lt;/codeph&gt;来观察后台任务的完成情况。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:75;483:1&#34;&gt;需要注意的是，分区是 DolphinDB database 存储数据的最小单位。DolphinDB对分区的写入操作是独占式的，当任务并行进行的时候，请避免多任务同时向一个分区写入数据。本例中每年的数据的写入由一个单独任务执行，各任务操作的数据范围没有重合，所以不可能发生多任务同时写入同一分区的情况。&lt;/p&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:76;485:1&#34;&gt;本案例的详细脚本在附录提供下载链接。&lt;/p&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;63-小文件批量导入&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:12;487:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:12;487:1&#34;&gt;6.3 小文件批量导入&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:12;487:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:77;489:1&#34;&gt;在现实场景中，数据供应商会将一只股票数据保存到一个文件中，这种场景的特点是文件数量比较多，但单个文件比较小。如果将文件一个个导入，则效率会比较低。为提高导入效率，可以考虑将多个小文件批量合并后再导入。数据文件：&lt;xref class=&#34;- topic/xref &#34; href=&#34;data/smallDataset.zip&#34; format=&#34;zip&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:7;489:106&#34;&gt;&lt;?ditaot usertext?&gt;数据文件&lt;/xref&gt;。示例脚本如下：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:34;491:1&#34;&gt;//1. 建库建表 database(directory = 'dfs://k_day_level', partitionType = RANGE, partitionScheme =[2000.01M,2001.01M,2002.01M,2003.01M,2004.01M,2005.01M,2006.01M,2007.01M,2008.01M,2009.01M,2010.01M,2011.01M,2012.01M,2013.01M,2014.01M,2015.01M,2016.01M,2017.01M,2018.01M,2019.01M,2020.01M,2021.01M,2022.01M,2023.01M,2024.01M]$7, engine= `OLAP, atomic = `TRANS) db = database(&#34;dfs://k_day_level&#34;) colName = `securityid`tradetime`open`close`high`low`vol`val`vwap colType = [SYMBOL,TIMESTAMP,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,DOUBLE,DOUBLE] tbSchema = table(1:0, colName, colType) if(existsDatabase(&#34;dfs://k_day_level&#34;)){ dropDatabase(&#34;dfs://k_day_level&#34;) } db.createPartitionedTable(table=tbSchema,tableName=`k_day,partitionColumns=`tradetime) //2. 导入数据文件 batchNum=1000 dir = &#34;/home/data/smallDataset/&#34; allFiles = files(dir).filename i = 0 s = allFiles.size() do{ files =allFiles[i:min(i+batchNum, s)] data = each(loadText, dir + files).unionAll(false) loadTable(&#34;dfs://k_day_level&#34;,&#34;k_day&#34;).append!(data) i = i + batchNum }while(i &amp;lt; s)&lt;/codeblock&gt;&lt;/body&gt;&lt;/topic&gt;&lt;topic class=&#34;- topic/topic &#34; ditaarch:DITAArchVersion=&#34;2.0&#34; specializations=&#34;@props/audience @props/deliveryTarget @props/otherprops @props/platform @props/product&#34; id=&#34;64-将数据文件导入表中并添加股票名称列&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;topic:13;517:1&#34; domains=&#34;a(props audience) a(props deliveryTarget) a(props otherprops) a(props platform) a(props product)&#34;&gt;&lt;title class=&#34;- topic/title &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;title:13;517:1&#34;&gt;6.4 将数据文件导入表中，并添加股票名称列&lt;/title&gt;&lt;body class=&#34;- topic/body &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;body:13;517:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:78;519:1&#34;&gt;和 6.3 的场景相似，一个股票的数据保存到一个文件，其中数据文件以标的名称命名，且文件中不包含标的名称列。本例导入需求如下：&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:10;521:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:33;521:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:79;521:3&#34;&gt;在导入数据时，需要将在数据表中添加一列用于存储标的名称。&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:34;523:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:80;523:3&#34;&gt;文件中的字段顺序和待导入表中字段顺序不一致。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:81;525:1&#34;&gt;本例以导入一个标的的文件为例，进行说明。&lt;/p&gt;&lt;ul class=&#34;- topic/ul &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;ul:11;527:1&#34;&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:35;527:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:82;527:3&#34;&gt;&lt;i class=&#34;+ topic/ph hi-d/i &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;i:1;527:3&#34;&gt;sz000001.csv&lt;/i&gt; 文件中字段顺序是 tradetime，open，close，high，low，vol。&lt;/p&gt;&lt;/li&gt;&lt;li class=&#34;- topic/li &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;li:36;529:1&#34;&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:83;529:3&#34;&gt;待导入表的字段顺序是 symbol（需要添加的列），datetime，vol，open，close，high，low。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p class=&#34;- topic/p &#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;p:84;531:1&#34;&gt;数据文件：&lt;xref class=&#34;- topic/xref &#34; href=&#34;data/import_data_06/sz000001.csv&#34; format=&#34;csv&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;xref:8;531:6&#34;&gt;&lt;?ditaot usertext?&gt;数据文件&lt;/xref&gt;。示例脚本如下：&lt;/p&gt;&lt;codeblock class=&#34;+ topic/pre pr-d/codeblock &#34; xml:space=&#34;preserve&#34; xtrf=&#34;file:/var/lib/jenkins/workspace/packDocCN/documentation/zh/tutorials/import_data.md&#34; xtrc=&#34;codeblock:35;533:1&#34;&gt;dir = &#34;D:/work/documentation/zh/tutorials/data/import_data_06/sz000001.csv&#34; sym = dir.split('/').tail(1).split('.')[0].')[0] if(existsDatabase(&#34;dfs://stock_data&#34;)) { dropDatabase(&#34;dfs://stock_data&#34;) } db=database(directory=&#34;dfs://stock_data&#34;, partitionType=VALUE, partitionScheme=2000.01M..2019.12M) colNames=`sym`tradetime`vol`open`close`high`low colTypes=[SYMBOL, DATETIME, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE] t = table(1:0, colNames, colTypes) pt = db.createPartitionedTable(t, `pt, `tradetime); def mytrans(mutable t, sym, colNames){ t.replaceColumn!(`tradetime, datetime(t.tradetime)) //在中添加第一列 sym t1 = select sym, * from t //通过 reorderColumns! 调整表中各列的顺序 t1.reorderColumns!(colNames) return t1 } loadTextEx(db, `pt, `tradetime, dir, transform=mytrans{,sym, colNames}) select top 10 * from loadTable(&#34;dfs://stock_data&#34;, `pt) | sym | tradetime | vol | open | close | high | low | |----------|---------------------|--------|---------|---------|---------|---------| | sz000001 | 2010.01.01T00:00:00 | 10,732 | 35.9484 | 35.4385 | 36.3261 | 35.9569 | | sz000001 | 2010.01.04T00:00:00 | 97,555 | 16.4653 | 13.6348 | 16.7255 | 14.9401 | | sz000001 | 2010.01.05T00:00:00 | 43,992 | 51.3616 | 53.1199 | 52.2155 | 50.4782 | | sz000001 | 2010.01.06T00:00:00 | 85,283 | 76.3469 | 80.0076 | 76.7017 | 74.9479 | | sz000001 | 2010.01.07T00:00:00 | 78,837 | 38.9411 | 35.8283 | 39.5269 | 38.2178 | | sz000001 | 2010.01.08T00:00:00 | 13,317 | 70.8803 | 67.9027 | 71.8693 | 70.8072 | | sz000001 | 2010.01.11T00:00:00 | 22,958 | 96.3163 | 97.4031 | 96.5605 | 96.364 | | sz000001 | 2010.01.12T00:00:00 | 45,621 | 17.2699 | 18.5016 | 17.7689 | 16.5028 | | sz000001 | 2010.01.13T00:00:00 | 54,886 | 75.7999 | 77.0245 | 76.4588 | 75.7482 | | sz000001 | 2010.01.14T00:00:00 | 3,132 | 49.8656 | 49.3416 | 50.7761 | 49.7972 | ## 7. 附录 - [CSV导入数据文件](data/candle_201801.csv) - [二进制导入例1数据文件](data/binSample.bin) - [二进制导入例2数据文件](data/binStringSample.bin) - [HDF5导入数据文件](data/candle_201801.h5) - [案例完整脚本](data/demoScript.txt)&lt;/codeblock&gt;&lt;/body&gt;&lt;/topic&gt;&lt;/topic&gt;&lt;/topic&gt;"/><meta name="wh-source-relpath" content="tutorials/import_data.md"/><meta name="wh-out-relpath" content="tutorials/import_data.html"/>

    <link rel="stylesheet" type="text/css" href="../oxygen-webhelp/app/commons.css?buildId=2024012323"/>
    <link rel="stylesheet" type="text/css" href="../oxygen-webhelp/app/topic.css?buildId=2024012323"/>

    <script src="../oxygen-webhelp/app/options/properties.js?buildId=20250305183303"></script>
    <script src="../oxygen-webhelp/app/localization/strings.js?buildId=2024012323"></script>
    <script src="../oxygen-webhelp/app/search/index/keywords.js?buildId=20250305183303"></script>
    <script defer="defer" src="../oxygen-webhelp/app/commons.js?buildId=2024012323"></script>
    <script defer="defer" src="../oxygen-webhelp/app/topic.js?buildId=2024012323"></script>
<link rel="stylesheet" type="text/css" href="../oxygen-webhelp/template/styles.css?buildId=2024012323"/><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script></head>

    <body id="数据导入概述" class="wh_topic_page frmBody">
        <a href="#wh_topic_body" class="sr-only sr-only-focusable">
            跳转到主要内容
        </a>
        
        
        
        
        <header class="navbar navbar-default wh_header">
    <div class="container-fluid">
        <div xmlns:whc="http://www.oxygenxml.com/webhelp/components" class="wh_header_flex_container navbar-nav navbar-expand-md navbar-dark">
            <div class="wh_logo_and_publication_title_container">
                <div class="wh_logo_and_publication_title">
                    
                    <a href="https://docs.dolphindb.cn/zh/index.html" class=" wh_logo d-none d-sm-block "><img src="../logo.png" alt="  DolphinDB 文档中心  "/></a>
                    <div class=" wh_publication_title "><a href="../index.html"><span class="booktitle">  <span class="ph mainbooktitle">DolphinDB 文档中心</span>  </span></a></div>
                    
                </div>
                
                
            </div>

            <div class="wh_top_menu_and_indexterms_link collapse navbar-collapse" id="wh_top_menu_and_indexterms_link">
                
                
                
                
            </div>
        <div class=" wh_search_input navbar-form wh_topic_page_search search " role="form">
            
            
            
            <form id="searchForm" method="get" role="search" action="../search.html"><div><input type="search" placeholder="搜索 " class="wh_search_textfield" id="textToSearch" name="searchQuery" aria-label="搜索查询" required="required"/><button type="submit" class="wh_search_button" aria-label="搜索"><span class="search_input_text">搜索</span></button></div></form>
            
            <script src="/vendors/react/umd/react.production.min.js" defer="defer"></script>
<script src="/vendors/react-dom/umd/react-dom.production.min.js" defer="defer"></script>
<script src="/vendors/dayjs/dayjs.min.js" defer="defer"></script>
<script src="/vendors/antd/dist/antd.min.js" defer="defer"></script>
<script src="/vendors/@ant-design/icons/dist/index.umd.min.js" defer="defer"></script>
<script src="/zh/index.js" type="module"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" defer="defer"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer="defer"><!--


--></script>
<script defer="defer"><!--

// 从主页重定向
const currentUrl = window.location.href;

// 判断当前URL是否包含index.html并且路径最后部分是index.html
if (currentUrl.endsWith('index.html')) {
    // 处理根目录下的index.html跳转
    const baseUrl = currentUrl.split('/index.html')[0]; // 获取index.html之前的部分
    const redirectUrl = `${baseUrl}/about/ddb_intro.html`; // 构建跳转路径
    window.location.href = redirectUrl; // 执行跳转
}

--></script>
            
        </div></div>
    </div>
</header>
        
        
         
        
        
        
        <div class="container-fluid" id="wh_topic_container">
            <div class="row">

                <nav class="wh_tools d-print-none navbar-expand-md" aria-label="Tools">
                    
                    <div data-tooltip-position="bottom" class=" wh_breadcrumb "></div>
                    
                    
                    
                    <div class="wh_right_tools">
                        <button class="wh_hide_highlight" aria-label="切换搜索突出显示" title="切换搜索突出显示"></button>
                        <button class="webhelp_expand_collapse_sections" data-next-state="collapsed" aria-label="折叠截面" title="折叠截面"></button>
                        
                        
                        
                        
                        <div class=" wh_print_link print d-none d-md-inline-block "><button onClick="window.print()" title="打印此页" aria-label="打印此页"></button></div>
                        
                        
                    </div>
                    
                </nav>
            </div>
            
            
            
            
            <div class="wh_content_area">
                <div class="row">
                    
                    
                    <div class="col-lg-10 col-md-10 col-sm-10 col-xs-12" id="wh_topic_body">
                        
                        <button id="wh_close_topic_toc_button" class="close-toc-button d-none" aria-label="Toggle topic table of content" aria-controls="wh_topic_toc" aria-expanded="true">
                            <span class="close-toc-icon-container">
                                <span class="close-toc-icon"></span>     
                            </span>
                        </button>
                        
                        <div class=" wh_topic_content body "><main role="main"><article class="- topic/topic topic" role="article" aria-labelledby="ariaid-title1"><h1 class="- topic/title title topictitle1" id="ariaid-title1">数据导入概述</h1><div class="- topic/body body"><p class="- topic/p p">DolphinDB提供了多种灵活的数据导入方法，来帮助用户方便的把海量数据从多个数据源导入。具体有如下4种途径：</p><ul class="- topic/ul ul"><li class="- topic/li li">通过文本文件导入</li><li class="- topic/li li">通过二进制文件导入</li><li class="- topic/li li">通过HDF5接口导入</li><li class="- topic/li li">通过ODBC接口导入</li></ul></div><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title2" id="1-dolphindb数据库基本概念和特点"><h2 class="- topic/title title topictitle2" id="ariaid-title2">1. DolphinDB数据库基本概念和特点</h2><div class="- topic/body body"><p class="- topic/p p">本章中多处使用到DolphinDB的数据库和表的概念，所以这里首先做一个介绍。</p><p class="- topic/p p">在DolphinDB里数据以结构化数据表的方式保存。数据表按存储介质可以分为：</p><ul class="- topic/ul ul"><li class="- topic/li li">内存表：数据保存在内存中，存取速度最快，但是若节点关闭就会丢失数据。</li><li class="- topic/li li">分布式表：数据分布在不同的节点的磁盘上，通过DolphinDB的分布式计算引擎，逻辑上仍然可以像本地表一样做统一查询。</li></ul><p class="- topic/p p">按是否分区可以分为：</p><ul class="- topic/ul ul"><li class="- topic/li li">普通表（未分区表）</li><li class="- topic/li li">分区表</li></ul><p class="- topic/p p">在传统的数据库系统，分区是针对数据表定义的，就是同一个数据库里的每个数据表都可以有自己的分区定义；而DolphinDB的分区是针对数据库定义的，也就是说同一个数据库下的数据表只能使用同一种分区机制，这也意味着如果两张表要使用不同的分区机制，那么它们是不能放在一个数据库下的。</p></div></article><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title3" id="2-通过文本文件导入"><h2 class="- topic/title title topictitle2" id="ariaid-title3">2. 通过文本文件导入</h2><div class="- topic/body body"><p class="- topic/p p">通过文件进行数据中转是比较通用化的一种数据迁移方式，方式简单易操作。DolphinDB提供了以下三个函数来载入文本文件：</p><ul class="- topic/ul ul"><li class="- topic/li li"><code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>: 将文本文件以 DolphinDB 数据表的形式读取到内存中。</li><li class="- topic/li li"><code class="+ topic/ph pr-d/codeph ph codeph">ploadText</code>: 将数据文件作为分区表并行加载到内存中。与<code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>函数相比，速度更快。</li><li class="- topic/li li"><code class="+ topic/ph pr-d/codeph ph codeph">loadTextEx</code>: 把数据文件转换为DolphinDB数据库中的分布式表，然后将表的元数据加载到内存中。</li></ul><p class="- topic/p p">下面通过将 <a class="- topic/xref xref" href="data/candle_201801.csv">candle_201801.csv</a> 导入DolphinDB来演示<code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>和<code class="+ topic/ph pr-d/codeph ph codeph">loadTextEx</code>的用法。</p></div><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title4" id="21-loadtext"><h3 class="- topic/title title topictitle3" id="ariaid-title4">2.1. <code class="+ topic/ph pr-d/codeph ph codeph">loadText</code></h3><div class="- topic/body body"><p class="- topic/p p"><code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>函数有三个参数，第一个参数filename是文件名，第二个参数delimiter用于指定不同字段的分隔符，默认是","，第三个参数schema是用来指定导入后表的每个字段的数据类型，schema参数是一个数据表，格式示例如下：</p><div class="table-container"><table class="- topic/table table" data-cols="2"><caption></caption><colgroup><col/><col/></colgroup><thead class="- topic/thead thead"><tr class="- topic/row"><th class="- topic/entry entry colsep-0 rowsep-0" id="21-loadtext__entry__1">name</th><th class="- topic/entry entry colsep-0 rowsep-0" id="21-loadtext__entry__2">type</th></tr></thead><tbody class="- topic/tbody tbody"><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__1">timestamp</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__2">SECOND</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__1">ID</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__2">INT</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__1">qty</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__2">INT</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__1">price</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__2">DOUBLE</td></tr></tbody></table></div><p class="- topic/p p">首先导入数据：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.csv"
tmpTB = loadText(dataFilePath);</code></pre><p class="- topic/p p">DolphinDB在导入数据的同时，随机提取一部分的行以确定各列数据类型，所以对大多数文本文件无须手动指定各列的数据类型，非常方便。但有时系统自动识别的数据类型并不符合预期或需求，比如导入数据的volume列被识别为INT类型, 而需要的volume类型是LONG类型，这时就需要使用一个数据类型表作为schema参数。例如可使用如下脚本构建数据类型表：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>nameCol = `symbol`exchange`cycle`tradingDay`date`time`open`high`low`close`volume`turnover`unixTime
typeCol = `SYMBOL`SYMBOL`INT`DATE`DATE`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE`LONG
schemaTb = table(nameCol as name,typeCol as type);</code></pre><p class="- topic/p p">当表字段非常多的时候，写这样一个脚本费时费力，为了简化操作，DolphinDB提供了<code class="+ topic/ph pr-d/codeph ph codeph">extractTextSchema</code> 函数，可从文本文件中提取表的结构生成数据类型表。只需修改少数指定字段的数据类型，就可得到理想的数据类型表。</p><p class="- topic/p p">整合上述方法，可使用如下脚本以导入数据：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.csv"
schemaTb=extractTextSchema(dataFilePath)
update schemaTb set type=`LONG where name=`volume        
tt=loadText(dataFilePath,,schemaTb);</code></pre><p class="- topic/p p">如果数据文件中包含时间、日期的数据，满足分隔符要求的这部分数据（日期数据分隔符包含"-"、"/"和"."，时间数据分隔符为":"）会解析为相应的类型。例如："10:56:16"被解析为SECOND，"2023-11-08"被解析为DATE类型。对于不包含分隔符的数据，形如"yyMMdd"的数据同时满足0&lt;=yy&lt;=99，0&lt;=MM&lt;=12，1&lt;=dd&lt;=31，会被优先解析成DATE；形如"yyyyMMdd"的数据同时满足1900&lt;=yyyy&lt;=2100，0&lt;=MM&lt;=12，1&lt;=dd&lt;=31会被优先解析成DATE。</p><p class="- topic/p p">以数据文件 test_time.csv 为例，date1-date8 不同格式的数据均被解析为 DATE 类型，second被解析为 SECOND 类型。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/test_time.csv"
schemaTable = extractTextSchema(dataFilePath)</code></pre><div class="table-container"><table class="- topic/table table" data-cols="2"><caption></caption><colgroup><col/><col/></colgroup><thead class="- topic/thead thead"><tr class="- topic/row"><th class="- topic/entry entry colsep-0 rowsep-0" id="21-loadtext__entry__11">name</th><th class="- topic/entry entry colsep-0 rowsep-0" id="21-loadtext__entry__12">type</th></tr></thead><tbody class="- topic/tbody tbody"><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date1</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date2</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date3</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date4</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date5</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date6</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date7</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">date8</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">DATE</td></tr><tr class="- topic/row"><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__11">second</td><td class="- topic/entry entry colsep-0 rowsep-0" headers="21-loadtext__entry__12">SECOND</td></tr></tbody></table></div><p class="- topic/p p">为确保数据导入准确，需要需要在 format 列中指定数据文件中日期或时间的格式</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/test_time.csv"
schemaTable = extractTextSchema(dataFilePath)
formatColumn = ["yyyy.MM.dd","yyyy/MM/dd","yyyy-MM-dd","yyyyMMdd","yy.MM.dd","yy/MM/dd","yy-MM-dd","yyMMdd","HH:mm:ss"]
schemaTable[`format] = formatColumn
t = loadText(dataFilePath,',',schemaTable)
### 2.2. `ploadText`

`ploadText`函数的特点可以快速载入大文件（至少16MB）。它在设计中充分利用了多核CPU来并行载入文件，并行程度取决于服务器本身CPU核数量和节点的workerNum配置。

首先通过脚本生成一个4G左右的CSV文件：

```txt
filePath = "/home/data/testFile.csv"
appendRows = 100000000
dateRange = 2010.01.01..2018.12.30
ints = rand(100, appendRows)
symbols = take(string('A'..'Z'), appendRows)
dates = take(dateRange, appendRows)
floats = rand(float(100), appendRows)
times = 00:00:00.000 + rand(60 * 60 * 24 * 1000, appendRows)
t = table(ints as int, symbols as symbol, dates as date, floats as float, times as time)
t.saveText(filePath)</code></pre><p class="- topic/p p">分别通过<code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>和<code class="+ topic/ph pr-d/codeph ph codeph">ploadText</code>来载入文件。本例所用节点是4核8超线程的CPU。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>timer loadText(filePath);
Time elapsed: 39728.393 ms

timer ploadText(filePath);
Time elapsed: 10685.838 ms</code></pre><p class="- topic/p p">结果显示在此配置下，<code class="+ topic/ph pr-d/codeph ph codeph">ploadText</code>的性能是<code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>的4倍左右。</p></div></article><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title5" id="23-loadtextex"><h3 class="- topic/title title topictitle3" id="ariaid-title5">2.3. <code class="+ topic/ph pr-d/codeph ph codeph">loadTextEx</code></h3><div class="- topic/body body"><p class="- topic/p p"><code class="+ topic/ph pr-d/codeph ph codeph">loadText</code>函数总是把所有数据导入内存。当数据文件体积非常庞大时，服务器的内存很容易成为制约因素。DolphinDB提供的<code class="+ topic/ph pr-d/codeph ph codeph">loadTextEx</code>函数可以较好的解决这个问题。它将一个大的文本文件分割成很多个小块，逐步加载到分布式数据表中。</p><p class="- topic/p p">首先创建分布式数据库：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>db=database("dfs://dataImportCSVDB",VALUE,2018.01.01..2018.01.31)  </code></pre><p class="- topic/p p">然后将文本文件导入数据库中"cycle"表：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.csv"
loadTextEx(db, "cycle", "tradingDay", dataFilePath)</code></pre><p class="- topic/p p">当需要使用数据时，通过<code class="+ topic/ph pr-d/codeph ph codeph">loadTable</code>函数将分区元数据先载入内存。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>tb = database("dfs://dataImportCSVDB").loadTable("cycle")</code></pre><p class="- topic/p p">在实际执行查询的时候，会按需加载所需数据到内存。</p></div></article></article><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title6" id="3-通过二进制文件导入"><h2 class="- topic/title title topictitle2" id="ariaid-title6">3. 通过二进制文件导入</h2><div class="- topic/body body"><p class="- topic/p p">对于二进制格式的文件，DolphinDB提供了2个函数用于导入：<code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数和<code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数。二者的区别是，前者不支持导入字符串类型的数据，后者支持。下面通过2个例子分别介绍这两个函数的用法。</p><ul class="- topic/ul ul"><li class="- topic/li li"><code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数</li></ul><p class="- topic/p p"><code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数能够导入不含有字符串类型字段的二进制文件，下面介绍如何使用<code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数导入一个二进制文件：<a class="- topic/xref xref" href="data/binSample.bin">binSample.bin</a>。</p><p class="- topic/p p">首先，创建一个内存表tb，用于存放导入的数据，需要为每一列指定字段名称和数据类型。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>tb=table(1000:0, `id`date`time`last`volume`value`ask1`ask_size1`bid1`bid_size1, [INT,INT,INT,FLOAT,INT,FLOAT,FLOAT,INT,FLOAT,INT])</code></pre><p class="- topic/p p">调用<code class="+ topic/ph pr-d/codeph ph codeph">file</code>函数打开文件，并通过<code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数导入二进制文件，数据会被加载到tb表中。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>dataFilePath="/home/data/binSample.bin"
f=file(dataFilePath)
f.readRecord!(tb);</code></pre><p class="- topic/p p">查看tb表的数据，数据已经正确导入：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>select top 5 * from tb;

id date     time     last volume value ask1  ask_size1 bid1  bid_size1
-- -------- -------- ---- ------ ----- ----- --------- ----- ---------
1  20190902 91804000 0    0      0     11.45 200       11.45 200
2  20190902 92007000 0    0      0     11.45 200       11.45 200
3  20190902 92046000 0    0      0     11.45 1200      11.45 1200
4  20190902 92346000 0    0      0     11.45 1200      11.45 1200
5  20190902 92349000 0    0      0     11.45 5100      11.45 5100</code></pre><p class="- topic/p p">date列和time列的数据为INT类型。可以使用<code class="+ topic/ph pr-d/codeph ph codeph">temporalParse</code>函数进行日期和时间类型数据的格式转换，再使用<code class="+ topic/ph pr-d/codeph ph codeph">replaceColumn!</code>函数替换表中原有的列。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>tb.replaceColumn!(`date, tb.date.string().temporalParse("yyyyMMdd"))
tb.replaceColumn!(`time, tb.time.format("000000000").temporalParse("HHmmssSSS"))
select top 5 * from tb;

id date       time         last volume value ask1  ask_size1 bid1  bid_size1
-- ---------- ------------ ---- ------ ----- ----- --------- ----- ---------
1  2019.09.02 09:18:04.000 0    0      0     11.45 200       11.45 200
2  2019.09.02 09:20:07.000 0    0      0     11.45 200       11.45 200
3  2019.09.02 09:20:46.000 0    0      0     11.45 1200      11.45 1200
4  2019.09.02 09:23:46.000 0    0      0     11.45 1200      11.45 1200
5  2019.09.02 09:23:49.000 0    0      0     11.45 5100      11.45 5100</code></pre><ul class="- topic/ul ul"><li class="- topic/li li"><code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数</li></ul><p class="- topic/p p"><code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数能够处理字符串类型的数据（包括STRING和SYMBOL类型），但是要求字符串在磁盘上的长度必须固定。如果字符串的长度小于固定值，则用ASCII值0填充，加载的时候会把末尾0去掉。下面介绍使用<code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数导入一个带有字符串类型字段的二进制文件：<a class="- topic/xref xref" href="data/binStringSample.bin">binStringSample.bin</a>。</p><p class="- topic/p p">首先，指定要导入文件的表结构，包括字段名称和数据类型。与<code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>函数不同的是，<code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数是通过一个元组来指定schema，而不是直接定义一个内存表。关于表结构的指定，有以下3点要求：</p><ol class="- topic/ol ol"><li class="- topic/li li">对于表中的每个字段，都需要以tuple的形式指定字段名称和相应的数据类型。</li><li class="- topic/li li">若类型是字符串，还需指定磁盘上的字符串长度（包括结尾的0）。例如：（"name",SYMBOL,24）。</li><li class="- topic/li li">将所有tuple按照字段顺序组成元组，作为表结构。</li></ol><p class="- topic/p p">针对本例中的数据文件指定表结构，具体如下所示。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>schema = [("code", SYMBOL, 32),("date", INT),("time", INT),("last", FLOAT),("volume", INT),("value", FLOAT),("ask1", FLOAT),("ask2", FLOAT),("ask3", FLOAT),("ask4", FLOAT),("ask5", FLOAT),("ask6", FLOAT),("ask7", FLOAT),("ask8", FLOAT),("ask9", FLOAT),("ask10", FLOAT),("ask_size1", INT),("ask_size2", INT),("ask_size3", INT),("ask_size4", INT),("ask_size5", INT),("ask_size6", INT),("ask_size7", INT),("ask_size8", INT),("ask_size9", INT),("ask_size10", INT),("bid1", FLOAT),("bid2", FLOAT),("bid3", FLOAT),("bid4", FLOAT),("bid5", FLOAT),("bid6", FLOAT),("bid7", FLOAT),("bid8", FLOAT),("bid9", FLOAT),("bid10", FLOAT),("bid_size1", INT),("bid_size2", INT),("bid_size3", INT),("bid_size4", INT),("bid_size5", INT),("bid_size6", INT),("bid_size7", INT),("bid_size8", INT),("bid_size9", INT),("bid_size10", INT)]</code></pre><p class="- topic/p p">使用<code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数导入二进制文件，由于表的列数较多，通过select语句选出几列有代表性的数据进行后面的介绍。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>dataFilePath="/home/data/binStringSample.bin"
tmp=loadRecord(dataFilePath, schema)
tb=select code,date,time,last,volume,value,ask1,ask_size1,bid1,bid_size1 from tmp;</code></pre><p class="- topic/p p">查看表内数据的前5行：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>select top 5 * from tb;

code      date     time     last volume value ask1  ask_size1 bid1  bid_size1
--------- -------- -------- ---- ------ ----- ----- --------- ----- ---------
601177.SH 20190902 91804000 0    0      0     11.45 200       11.45 200
601177.SH 20190902 92007000 0    0      0     11.45 200       11.45 200
601177.SH 20190902 92046000 0    0      0     11.45 1200      11.45 1200
601177.SH 20190902 92346000 0    0      0     11.45 1200      11.45 1200
601177.SH 20190902 92349000 0    0      0     11.45 5100      11.45 5100</code></pre><p class="- topic/p p">处理日期和时间列的数据：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>tb.replaceColumn!(`date, tb.date.string().temporalParse("yyyyMMdd"))
tb.replaceColumn!(`time, tb.time.format("000000000").temporalParse("HHmmssSSS"))
select top 5 * from tb;

code      date       time         last volume value ask1  ask_size1 bid1  bid_size1
--------- ---------- ------------ ---- ------ ----- ----- --------- ----- ---------
601177.SH 2019.09.02 09:18:04.000 0    0      0     11.45 200       11.45 200
601177.SH 2019.09.02 09:20:07.000 0    0      0     11.45 200       11.45 200
601177.SH 2019.09.02 09:20:46.000 0    0      0     11.45 1200      11.45 1200
601177.SH 2019.09.02 09:23:46.000 0    0      0     11.45 1200      11.45 1200
601177.SH 2019.09.02 09:23:49.000 0    0      0     11.45 5100      11.45 5100</code></pre><p class="- topic/p p">除了<code class="+ topic/ph pr-d/codeph ph codeph">readRecord!</code>和<code class="+ topic/ph pr-d/codeph ph codeph">loadRecord</code>函数之外，DolphinDB还提供了一些与二进制文件的处理相关的函数，例如<code class="+ topic/ph pr-d/codeph ph codeph">writeRecord</code>函数，用于将DolphinDB对象保存为二进制文件。具体请参考用户手册。</p></div></article><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title7" id="4-通过hdf5接口导入"><h2 class="- topic/title title topictitle2" id="ariaid-title7">4. 通过HDF5接口导入</h2><div class="- topic/body body"><p class="- topic/p p">HDF5是一种高效的二进制数据文件格式，在数据分析领域广泛使用。DolphinDB支持导入HDF5格式数据文件。</p><p class="- topic/p p">DolphinDB通过HDF5插件来访问HDF5文件，插件提供了以下方法：</p><ul class="- topic/ul ul"><li class="- topic/li li"><p class="- topic/p p">hdf5::ls - 列出h5文件中所有 Group 和 Dataset 对象</p></li><li class="- topic/li li"><p class="- topic/p p">hdf5::lsTable - 列出h5文件中所有 Dataset 对象</p></li><li class="- topic/li li"><p class="- topic/p p">hdf5::hdf5DS - 返回h5文件中 Dataset 的元数据</p></li><li class="- topic/li li"><p class="- topic/p p">hdf5::loadHDF5 - 将h5文件导入内存表</p></li><li class="- topic/li li"><p class="- topic/p p">hdf5::loadHDF5Ex - 将h5文件导入分区表</p></li><li class="- topic/li li"><p class="- topic/p p">hdf5::extractHDF5Schema - 从h5文件中提取表结构</p></li></ul><p class="- topic/p p">DolphinDB 1.00.0版本之后，安装目录/server/plugins/hdf5已经包含HDF5插件，使用以下脚本加载插件：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>loadPlugin("plugins/hdf5/PluginHdf5.txt")</code></pre><p class="- topic/p p">若用户使用的是老版本，默认不包含此插件，可先从<a class="- topic/xref xref" href="../plugins/hdf5/hdf5.html">HDF5插件</a>对应版本分支bin目录下载，再将插件部署到节点的plugins目录下。</p><p class="- topic/p p">调用插件方法时需要在方法前面提供namespace，比如调用loadHDF5可以使用<code class="+ topic/ph pr-d/codeph ph codeph">hdf5::loadHDF5</code>。另一种写法是：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>use hdf5
loadHDF5(filePath,tableName)</code></pre><p class="- topic/p p">HDF5文件的导入与CSV文件类似。例如，若要导入包含一个Dataset candle_201801的文件candle_201801.h5，可使用以下脚本，其中datasetName可通过ls或lsTable获得：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.h5"
datasetName = "candle_201801"
tmpTB = hdf5::loadHDF5(dataFilePath,datasetName)</code></pre><p class="- topic/p p">如果需要指定数据类型导入可以使用<code class="+ topic/ph pr-d/codeph ph codeph">hdf5::extractHDF5Schema</code>，脚本如下：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.h5"
datasetName = "candle_201801"
schema=hdf5::extractHDF5Schema(dataFilePath,datasetName)
update schema set type=`LONG where name=`volume
tt=hdf5::loadHDF5(dataFilePath,datasetName,schema)</code></pre><p class="- topic/p p">如果HDF5文件超过服务器内存，可以使用<code class="+ topic/ph pr-d/codeph ph codeph">hdf5::loadHDF5Ex</code>载入数据。</p><p class="- topic/p p">首先创建用于保存数据的分布式表：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>dataFilePath = "/home/data/candle_201801.h5"
datasetName = "candle_201801"
dfsPath = "dfs://dataImportHDF5DB"
db=database(dfsPath,VALUE,2018.01.01..2018.01.31)  </code></pre><p class="- topic/p p">然后导入HDF5文件：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>hdf5::loadHDF5Ex(db, "cycle", "tradingDay", dataFilePath,datasetName)</code></pre></div></article><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title8" id="5-通过odbc接口导入"><h2 class="- topic/title title topictitle2" id="ariaid-title8">5. 通过ODBC接口导入</h2><div class="- topic/body body"><p class="- topic/p p">DolphinDB支持ODBC接口连接第三方数据库，从其中直接将数据表读取成DolphinDB的内存数据表。</p><p class="- topic/p p">DolphinDB官方提供ODBC插件用于连接第三方数据源，使用该插件可以方便的从ODBC支持的数据库迁移数据至DolphinDB中。</p><p class="- topic/p p">ODBC插件提供了以下四个方法用于操作第三方数据源数据：</p><ul class="- topic/ul ul"><li class="- topic/li li">odbc::connect - 开启连接</li><li class="- topic/li li">odbc::close - 关闭连接</li><li class="- topic/li li">odbc::query - 根据给定的SQL语句查询数据并将结果返回到DolphinDB的内存表</li><li class="- topic/li li">odbc::execute - 在第三方数据库内执行给定的SQL语句，不返回结果。</li><li class="- topic/li li">odbc::append - 把DolphinDB中表的数据写入第三方数据库的表中。</li></ul><p class="- topic/p p">在使用ODBC插件之前，需要安装ODBC驱动程序，请参考<a class="- topic/xref xref" href="../plugins/odbc/odbc.html">ODBC插件使用教程</a>。</p><p class="- topic/p p">下面的例子使用ODBC插件连接以下SQL Server：</p><ul class="- topic/ul ul"><li class="- topic/li li">server：172.18.0.15</li><li class="- topic/li li">默认端口：1433</li><li class="- topic/li li">连接用户名：sa</li><li class="- topic/li li">密码：123456</li><li class="- topic/li li">数据库名称： SZ_TAQ</li></ul><p class="- topic/p p">第一步，下载插件解压并拷贝 plugins\odbc 目录下所有文件到DolphinDB server的 plugins/odbc 目录下（有些版本的DolphinDB安装目录/server/plugins/odbc已经包含ODBC插件，可略过此步），通过下面的脚本完成插件初始化：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>loadPlugin("plugins/odbc/odbc.cfg")
conn=odbc::connect("Driver=ODBC Driver 17 for SQL Server;Server=172.18.0.15;Database=SZ_TAQ;Uid=sa;Pwd=123456;")</code></pre><p class="- topic/p p">第二步，创建分布式数据库。使用SQL Server中的数据表结构作为DolphinDB数据表的模板。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>tb = odbc::query(conn,"select top 1 * from candle_201801")
db=database("dfs://dataImportODBC",VALUE,2018.01.01..2018.01.31)
db.createPartitionedTable(tb, "cycle", "tradingDay")</code></pre><p class="- topic/p p">第三步，从SQL Server中导入数据并保存为DolphinDB分区表：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>tb = database("dfs://dataImportODBC").loadTable("cycle")
data = odbc::query(conn,"select * from candle_201801")
tb.append!(data);</code></pre><p class="- topic/p p">通过ODBC导入数据方便快捷。通过DolphinDB的定时作业机制，它还可以作为时序数据定时同步的数据通道。</p></div></article><article class="- topic/topic topic nested1" aria-labelledby="ariaid-title9" id="6-导入数据实例"><h2 class="- topic/title title topictitle2" id="ariaid-title9">6. 导入数据实例</h2><div class="- topic/body body"><p class="- topic/p p">下面以股票市场日K线图数据文件导入作为示例。每个股票数据存为一个CSV文件，共约100G，时间范围为2008年-2017年，按年度分目录保存。2008年度路径示例如下：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>2008
    ---- 000001.csv
    ---- 000002.csv
    ---- 000003.csv
    ---- 000004.csv
    ---- ...</code></pre><p class="- topic/p p">每个文件的结构都是一致的，如图所示：</p><br/><img class="- topic/image image" src="images/csvfile.PNG" alt="csvfile"/><br/></div><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title10" id="61-分区规划"><h3 class="- topic/title title topictitle3" id="ariaid-title10">6.1. 分区规划</h3><div class="- topic/body body"><p class="- topic/p p">要导入数据之前，首先要做好数据的分区规划，即确定分区字段以及分区粒度。</p><p class="- topic/p p">确定分区字段要考虑日常的查询语句执行频率。以where, group by或context by中常用字段作为分区字段，可以极大的提升数据检索和分析的效率。使用股票数据的查询经常与交易日期和股票代码有关，所以我们建议采用
tradingDay和symbol这两列进行组合(COMPO)分区。</p><p class="- topic/p p">分区大小应尽量均匀，同时分区粒度不宜过大或过小。我们建议一个分区未压缩前的原始数据大小控制在100M~1G之间。有关为何分区大小应均匀，以及分区最佳粒度的考虑因素，请参考<a class="- topic/xref xref" href="database.html">DolphinDB分区数据库教程</a>第四节。</p><p class="- topic/p p">综合考虑，我们可以在复合(COMPO)分区中，根据交易日期进行范围分区（每年一个范围），并按照股票代码进行范围分区（共100个代码范围），共产生 10 * 100 = 1000 个分区，最终每个分区的大小约100M左右。</p><p class="- topic/p p">首先创建交易日期的分区向量。若要为后续进入的数据预先制作分区，可把时间范围设置为2008-2030年。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>yearRange = date(2008.01M + 12*0..22);</code></pre><p class="- topic/p p">通过以下脚本得到symbol字段的分区向量。由于每只股票的数据量一致，我们遍历所有的年度目录，整理出股票代码清单，并通过<code class="+ topic/ph pr-d/codeph ph codeph">cutPoint</code>函数分成100个股票代码区间。考虑到未来新增的股票代码可能会大于现有最大股票代码，我们增加了一个虚拟的代码999999，作为股票代码的上限值。</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>symbols = array(SYMBOL, 0, 100)
yearDirs = files(rootDir)[`filename]
for(yearDir in yearDirs){
	path = rootDir + "/" + yearDir
	symbols.append!(files(path)[`filename].upper().strReplace(".CSV",""))
}
symbols = symbols.distinct().sort!().append!("999999");
symRanges = symbols.cutPoints(100)</code></pre><p class="- topic/p p">通过以下脚本创建复合(COMPO)分区数据库，以及数据库内的分区表"stockData"：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>columns=`symbol`exchange`cycle`tradingDay`date`time`open`high`low`close`volume`turnover`unixTime
types =  [SYMBOL,SYMBOL,INT,DATE,DATE,TIME,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG,DOUBLE,LONG]

dbDate=database("", RANGE, yearRange)
dbID=database("", RANGE, symRanges)
db = database(dbPath, COMPO, [dbDate, dbID])

pt=db.createPartitionedTable(table(1000000:0,columns,types), `stockData, `tradingDay`symbol);</code></pre></div></article><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title11" id="62-导入数据"><h3 class="- topic/title title topictitle3" id="ariaid-title11">6.2. 导入数据</h3><div class="- topic/body body"><p class="- topic/p p">数据导入的具体过程是通过目录树，将所有的CSV文件读取并写入到分布式数据库表dfs://SAMPLE_TRDDB 中。这其中会有一些细节问题。例如，CSV文件中保存的数据格式与DolphinDB内部的数据格式存在差异，比如time字段，原始数据文件里是以整数例如“9390100000”表示精确到毫秒的时间， 如果直接读入会被识别成整数类型，而不是时间类型，所以这里需要用到数据转换函数<code class="+ topic/ph pr-d/codeph ph codeph">datetimeParse</code>结合格式化函数<code class="+ topic/ph pr-d/codeph ph codeph">format</code>在数据导入时进行转换。可采用以下脚本：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>datetimeParse(format(time,"000000000"),"HHmmssSSS")</code></pre><p class="- topic/p p">如果单线程导入100GB的数据会耗时很久。为了充分利用集群的资源，我们可以按照年度把数据导入拆分成多个子任务，发送到各节点的任务队列并行执行，提高导入的效率。这个过程可分为以下两步实现。</p><p class="- topic/p p">首先定义一个函数以导入指定年度目录下的所有文件：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>def loadCsvFromYearPath(path, dbPath, tableName){
	symbols = files(path)[`filename]
	for(sym in symbols){
		filePath = path + "/" + sym
		t=loadText(filePath)
		database(dbPath).loadTable(tableName).append!(select symbol, exchange,cycle, tradingDay,date,datetimeParse(format(time,"000000000"),"HHmmssSSS"),open,high,low,close,volume,turnover,unixTime from t )
    }
}</code></pre><p class="- topic/p p">然后通过<code class="+ topic/ph pr-d/codeph ph codeph">rpc</code>函数结合<code class="+ topic/ph pr-d/codeph ph codeph">submitJob</code>函数把该函数提交到各节点去执行：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock txt"><code>nodesAlias="NODE" + string(1..4)
years= files(rootDir)[`filename]

index = 0;
for(year in years){	
	yearPath = rootDir + "/" + year
	des = "loadCsv_" + year
	rpc(nodesAlias[index%nodesAlias.size()],submitJob,des,des,loadCsvFromYearPath,yearPath,dbPath,`stockData)
	index=index+1
}</code></pre><p class="- topic/p p">数据导入过程中，可以使用<code class="+ topic/ph pr-d/codeph ph codeph">pnodeRun(getRecentJobs)</code>来观察后台任务的完成情况。</p><p class="- topic/p p">需要注意的是，分区是 DolphinDB database 存储数据的最小单位。DolphinDB对分区的写入操作是独占式的，当任务并行进行的时候，请避免多任务同时向一个分区写入数据。本例中每年的数据的写入由一个单独任务执行，各任务操作的数据范围没有重合，所以不可能发生多任务同时写入同一分区的情况。</p><p class="- topic/p p">本案例的详细脚本在附录提供下载链接。</p></div></article><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title12" id="63-小文件批量导入"><h3 class="- topic/title title topictitle3" id="ariaid-title12">6.3 小文件批量导入</h3><div class="- topic/body body"><p class="- topic/p p">在现实场景中，数据供应商会将一只股票数据保存到一个文件中，这种场景的特点是文件数量比较多，但单个文件比较小。如果将文件一个个导入，则效率会比较低。为提高导入效率，可以考虑将多个小文件批量合并后再导入。数据文件：<a class="- topic/xref xref" href="data/smallDataset.zip">数据文件</a>。示例脚本如下：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>//1. 建库建表
database(directory = 'dfs://k_day_level', partitionType = RANGE, partitionScheme =[2000.01M,2001.01M,2002.01M,2003.01M,2004.01M,2005.01M,2006.01M,2007.01M,2008.01M,2009.01M,2010.01M,2011.01M,2012.01M,2013.01M,2014.01M,2015.01M,2016.01M,2017.01M,2018.01M,2019.01M,2020.01M,2021.01M,2022.01M,2023.01M,2024.01M]$7, engine= `OLAP, atomic = `TRANS)

db = database("dfs://k_day_level")
colName = `securityid`tradetime`open`close`high`low`vol`val`vwap
colType = [SYMBOL,TIMESTAMP,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,DOUBLE,DOUBLE]
tbSchema = table(1:0, colName, colType)
if(existsDatabase("dfs://k_day_level")){
	dropDatabase("dfs://k_day_level")
}
db.createPartitionedTable(table=tbSchema,tableName=`k_day,partitionColumns=`tradetime)

//2. 导入数据文件
batchNum=1000 
dir = "/home/data/smallDataset/"
allFiles = files(dir).filename
i = 0
s = allFiles.size()
do{
    files =allFiles[i:min(i+batchNum, s)]
    data = each(loadText, dir + files).unionAll(false)
    loadTable("dfs://k_day_level","k_day").append!(data)
    i = i + batchNum
}while(i &lt; s)</code></pre></div></article><article class="- topic/topic topic nested2" aria-labelledby="ariaid-title13" id="64-将数据文件导入表中并添加股票名称列"><h3 class="- topic/title title topictitle3" id="ariaid-title13">6.4 将数据文件导入表中，并添加股票名称列</h3><div class="- topic/body body"><p class="- topic/p p">和 6.3 的场景相似，一个股票的数据保存到一个文件，其中数据文件以标的名称命名，且文件中不包含标的名称列。本例导入需求如下：</p><ul class="- topic/ul ul"><li class="- topic/li li"><p class="- topic/p p">在导入数据时，需要将在数据表中添加一列用于存储标的名称。</p></li><li class="- topic/li li"><p class="- topic/p p">文件中的字段顺序和待导入表中字段顺序不一致。</p></li></ul><p class="- topic/p p">本例以导入一个标的的文件为例，进行说明。</p><ul class="- topic/ul ul"><li class="- topic/li li"><p class="- topic/p p"><em class="+ topic/ph hi-d/i ph i">sz000001.csv</em> 文件中字段顺序是 tradetime，open，close，high，low，vol。</p></li><li class="- topic/li li"><p class="- topic/p p">待导入表的字段顺序是 symbol（需要添加的列），datetime，vol，open，close，high，low。</p></li></ul><p class="- topic/p p">数据文件：<a class="- topic/xref xref" href="data/import_data_06/sz000001.csv">数据文件</a>。示例脚本如下：</p><pre class="+ topic/pre pr-d/codeblock pre codeblock"><code>dir = "D:/work/documentation/zh/tutorials/data/import_data_06/sz000001.csv"
sym = dir.split('/').tail(1).split('.')[0].')[0]

if(existsDatabase("dfs://stock_data")) {
	dropDatabase("dfs://stock_data")
}

db=database(directory="dfs://stock_data", partitionType=VALUE, partitionScheme=2000.01M..2019.12M)

colNames=`sym`tradetime`vol`open`close`high`low
colTypes=[SYMBOL, DATETIME, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE]
t = table(1:0, colNames, colTypes)
pt = db.createPartitionedTable(t, `pt, `tradetime);
def mytrans(mutable t, sym, colNames){
        t.replaceColumn!(`tradetime, datetime(t.tradetime))
        //在中添加第一列 sym
        t1 = select sym, * from t
        //通过 reorderColumns! 调整表中各列的顺序 
        t1.reorderColumns!(colNames)
        return t1
}

loadTextEx(db, `pt, `tradetime, dir, transform=mytrans{,sym, colNames})

select top 10 * from loadTable("dfs://stock_data", `pt)

| sym      | tradetime           | vol    | open    | close   | high    | low     |
|----------|---------------------|--------|---------|---------|---------|---------|
| sz000001 | 2010.01.01T00:00:00 | 10,732 | 35.9484 | 35.4385 | 36.3261 | 35.9569 |
| sz000001 | 2010.01.04T00:00:00 | 97,555 | 16.4653 | 13.6348 | 16.7255 | 14.9401 |
| sz000001 | 2010.01.05T00:00:00 | 43,992 | 51.3616 | 53.1199 | 52.2155 | 50.4782 |
| sz000001 | 2010.01.06T00:00:00 | 85,283 | 76.3469 | 80.0076 | 76.7017 | 74.9479 |
| sz000001 | 2010.01.07T00:00:00 | 78,837 | 38.9411 | 35.8283 | 39.5269 | 38.2178 |
| sz000001 | 2010.01.08T00:00:00 | 13,317 | 70.8803 | 67.9027 | 71.8693 | 70.8072 |
| sz000001 | 2010.01.11T00:00:00 | 22,958 | 96.3163 | 97.4031 | 96.5605 | 96.364  |
| sz000001 | 2010.01.12T00:00:00 | 45,621 | 17.2699 | 18.5016 | 17.7689 | 16.5028 |
| sz000001 | 2010.01.13T00:00:00 | 54,886 | 75.7999 | 77.0245 | 76.4588 | 75.7482 |
| sz000001 | 2010.01.14T00:00:00 | 3,132  | 49.8656 | 49.3416 | 50.7761 | 49.7972 |

## 7. 附录

- [CSV导入数据文件](data/candle_201801.csv)
- [二进制导入例1数据文件](data/binSample.bin)
- [二进制导入例2数据文件](data/binStringSample.bin)
- [HDF5导入数据文件](data/candle_201801.h5)
- [案例完整脚本](data/demoScript.txt)</code></pre></div></article></article></article></main></div>
                        
                        
                        
                        
                        
                        
                    </div>
                    
                        <nav role="navigation" id="wh_topic_toc" aria-label="On this page" class="col-lg-2 d-none d-lg-block navbar d-print-none"> 
                            <div id="wh_topic_toc_content">
		                        
	                            <div class=" wh_topic_toc "><div class="wh_topic_label">在本页上</div><ul><li class="topic-item"><a href="#1-dolphindb%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E7%89%B9%E7%82%B9" data-tocid="1-dolphindb数据库基本概念和特点">1. DolphinDB数据库基本概念和特点</a></li><li class="topic-item"><a href="#2-%E9%80%9A%E8%BF%87%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5" data-tocid="2-通过文本文件导入">2. 通过文本文件导入</a><ul><li class="topic-item"><a href="#21-loadtext" data-tocid="21-loadtext">2.1. <code class="+ topic/ph pr-d/codeph ph codeph">loadText</code></a></li><li class="topic-item"><a href="#23-loadtextex" data-tocid="23-loadtextex">2.3. <code class="+ topic/ph pr-d/codeph ph codeph">loadTextEx</code></a></li></ul></li><li class="topic-item"><a href="#3-%E9%80%9A%E8%BF%87%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5" data-tocid="3-通过二进制文件导入">3. 通过二进制文件导入</a></li><li class="topic-item"><a href="#4-%E9%80%9A%E8%BF%87hdf5%E6%8E%A5%E5%8F%A3%E5%AF%BC%E5%85%A5" data-tocid="4-通过hdf5接口导入">4. 通过HDF5接口导入</a></li><li class="topic-item"><a href="#5-%E9%80%9A%E8%BF%87odbc%E6%8E%A5%E5%8F%A3%E5%AF%BC%E5%85%A5" data-tocid="5-通过odbc接口导入">5. 通过ODBC接口导入</a></li><li class="topic-item"><a href="#6-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%AE%9E%E4%BE%8B" data-tocid="6-导入数据实例">6. 导入数据实例</a><ul><li class="topic-item"><a href="#61-%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%92" data-tocid="61-分区规划">6.1. 分区规划</a></li><li class="topic-item"><a href="#62-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE" data-tocid="62-导入数据">6.2. 导入数据</a></li><li class="topic-item"><a href="#63-%E5%B0%8F%E6%96%87%E4%BB%B6%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5" data-tocid="63-小文件批量导入">6.3 小文件批量导入</a></li><li class="topic-item"><a href="#64-%E5%B0%86%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%AF%BC%E5%85%A5%E8%A1%A8%E4%B8%AD%E5%B9%B6%E6%B7%BB%E5%8A%A0%E8%82%A1%E7%A5%A8%E5%90%8D%E7%A7%B0%E5%88%97" data-tocid="64-将数据文件导入表中并添加股票名称列">6.4 将数据文件导入表中，并添加股票名称列</a></li></ul></li></ul></div>
	                        	
                        	</div>
                        </nav>
                    
                </div>
            </div>
            
            
            
        </div> 
        <footer class="navbar navbar-default wh_footer">
  <div class=" footer-container mx-auto ">
<title>Copyright</title><p><b> ©2025 浙江智臾科技有限公司 浙ICP备18048711号-3</b></p>
  </div>
</footer>
        
        <div id="go2top" class="d-print-none">
            <span class="oxy-icon oxy-icon-up"></span>
        </div>
        
        <div id="modal_img_large" class="modal">
            <span class="close oxy-icon oxy-icon-remove"></span>
            <div id="modal_img_container"></div>
            <div id="caption"></div>
        </div>
        
        
        
    </body>
</html>